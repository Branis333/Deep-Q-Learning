{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142bd93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 5.65GB > 4.90GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from models\\dqn_pong_cnn.zip. Eval reward over 3 episodes: -17.00 ± 1.63\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load existing trained model (if present) and verify environment\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import ale_py  # ensure ALE namespace is registered\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "MODEL_PATH = os.path.join(\"models\", \"dqn_pong_cnn.zip\")\n",
    "ENV_ID = \"ALE/Pong-v5\"\n",
    "\n",
    "# Recreate the same preprocessing used for CNN training: make_atari_env + 4-frame stack\n",
    "try:\n",
    "    env = make_atari_env(ENV_ID, n_envs=1, seed=42)\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to build Atari VecEnv for {ENV_ID}. Ensure ale-py and ROMs are installed. Original error: {e}\"\n",
    "    )\n",
    "\n",
    "if os.path.isfile(MODEL_PATH):\n",
    "    model = DQN.load(MODEL_PATH, env=env)\n",
    "    mean_r, std_r = evaluate_policy(model, env, n_eval_episodes=3)\n",
    "    print(f\"Loaded model from {MODEL_PATH}. Eval reward over 3 episodes: {mean_r:.2f} ± {std_r:.2f}\")\n",
    "else:\n",
    "    model = None\n",
    "    print(f\"No existing model found at {MODEL_PATH}. You can still run experiments below.\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b55d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function ready. Configure hyperparameter sets in next cell.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Reusable training function for hyperparameter experiments (CNN policy)\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Callback to record episode stats into a unique CSV per run\n",
    "class EpisodeCSVLogger(BaseCallback):\n",
    "    def __init__(self, csv_path: str, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.csv_path = csv_path\n",
    "        self.rows = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # VecEnv infos may contain 'episode' dict on episode end\n",
    "        for info in self.locals.get(\"infos\", []):\n",
    "            if \"episode\" in info:\n",
    "                ep = info[\"episode\"]\n",
    "                self.rows.append((self.num_timesteps, ep.get(\"l\", None), ep.get(\"r\", None)))\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        import csv, os\n",
    "        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n",
    "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"timestep\", \"ep_length\", \"ep_reward\"]) \n",
    "            w.writerows(self.rows)\n",
    "        if self.verbose:\n",
    "            print(f\"Saved episode CSV to {self.csv_path}\")\n",
    "\n",
    "\n",
    "def make_cnn_env(seed: int):\n",
    "    env = make_atari_env(ENV_ID, n_envs=1, seed=seed)\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    return env\n",
    "\n",
    "\n",
    "def train_experiment(name: str, hp: Dict, total_timesteps: int = 50_000, seed: int = 42, eval_episodes: int = 3) -> Tuple[Dict, DQN]:\n",
    "    \"\"\"Train a DQN(CnnPolicy) model with given hyperparameters.\n",
    "    Returns (metrics_dict, model) WITHOUT saving the model; caller decides which to persist.\n",
    "    hp is a dict of DQN constructor keyword args (learning_rate, gamma, batch_size, etc.).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    os.makedirs(\"models\", exist_ok=True)  # ensure exists for later best-model save\n",
    "\n",
    "    env = make_cnn_env(seed)\n",
    "    csv_log = os.path.join(\"logs\", f\"training_metrics_{name}.csv\")\n",
    "    callback = EpisodeCSVLogger(csv_log, verbose=0)\n",
    "\n",
    "    model = DQN(\n",
    "        \"CnnPolicy\",\n",
    "        env,\n",
    "        seed=seed,\n",
    "        tensorboard_log=os.path.join(\"logs\", \"tensorboard\", name),\n",
    "        **hp,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[RUN {name}] Training {total_timesteps} steps | hp={hp}\")\n",
    "    t0 = time.time()\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n",
    "    minutes = (time.time() - t0) / 60.0\n",
    "\n",
    "    mean_r, std_r = evaluate_policy(model, env, n_eval_episodes=eval_episodes)\n",
    "    env.close()\n",
    "\n",
    "    metrics = {\n",
    "        \"name\": name,\n",
    "        \"mean_reward\": float(mean_r),\n",
    "        \"std_reward\": float(std_r),\n",
    "        \"train_minutes\": minutes,\n",
    "        **hp,\n",
    "    }\n",
    "    print(f\"[RUN {name}] Finished: mean_reward={mean_r:.2f} ± {std_r:.2f} | train_minutes={minutes:.2f}\")\n",
    "    return metrics, model\n",
    "\n",
    "print(\"Training function ready. Configure hyperparameter sets in next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86a7dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 10 experiment configs.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define TEN hyperparameter experiment configurations (CNN policy)\n",
    "# Expanded from 5 to 10 for broader tuning coverage.\n",
    "# Strategy: vary learning rate, buffer size, batch size, gamma, exploration schedule, and target update interval.\n",
    "\n",
    "experiments = [\n",
    "    # 1 Baseline (close to defaults)\n",
    "    {\n",
    "        \"name\": \"exp1_baseline\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            buffer_size=100_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=10_000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 2 Larger batch size + slower LR\n",
    "    {\n",
    "        \"name\": \"exp2_large_batch_slow_lr\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=5e-5,\n",
    "            gamma=0.99,\n",
    "            batch_size=64,\n",
    "            buffer_size=150_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=8_000,\n",
    "            exploration_fraction=0.15,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.02,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 3 Higher gamma, faster target updates\n",
    "    {\n",
    "        \"name\": \"exp3_high_gamma_fast_target\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1.5e-4,\n",
    "            gamma=0.995,\n",
    "            batch_size=32,\n",
    "            buffer_size=120_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=5_000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 4 Aggressive exploration decay (short fraction)\n",
    "    {\n",
    "        \"name\": \"exp4_aggressive_exploration_decay\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            buffer_size=100_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=10_000,\n",
    "            exploration_fraction=0.05,  # quicker decay\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 5 Larger buffer, slightly higher LR\n",
    "    {\n",
    "        \"name\": \"exp5_large_buffer_higher_lr\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=2e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            buffer_size=200_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=10_000,\n",
    "            exploration_fraction=0.12,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 6 Smaller buffer, faster updates, higher exploration fraction\n",
    "    {\n",
    "        \"name\": \"exp6_small_buffer_fast_updates\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            buffer_size=50_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=4_000,\n",
    "            exploration_fraction=0.2,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.02,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 7 Higher batch size + higher learning rate + slower exploration decay\n",
    "    {\n",
    "        \"name\": \"exp7_high_lr_high_batch\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=3e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=64,\n",
    "            buffer_size=150_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=12_000,\n",
    "            exploration_fraction=0.18,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.02,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 8 Very high gamma (more credit assignment) + moderate LR\n",
    "    {\n",
    "        \"name\": \"exp8_very_high_gamma\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.997,\n",
    "            batch_size=32,\n",
    "            buffer_size=120_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=8_000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 9 Faster train freq (every 1 step) smaller batch\n",
    "    {\n",
    "        \"name\": \"exp9_freq1_small_batch\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=16,\n",
    "            buffer_size=100_000,\n",
    "            train_freq=1,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=10_000,\n",
    "            exploration_fraction=0.12,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "    # 10 Larger gradient steps per update\n",
    "    {\n",
    "        \"name\": \"exp10_more_gradient_steps\",\n",
    "        \"hp\": dict(\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            buffer_size=100_000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=4,  # perform multiple gradient steps per collection\n",
    "            target_update_interval=10_000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.01,\n",
    "            verbose=0,\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(experiments)} experiment configs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856fa071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 5.65GB > 4.85GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RUN exp1_baseline] Training 50000 steps | hp={'learning_rate': 0.0001, 'gamma': 0.99, 'batch_size': 32, 'buffer_size': 100000, 'train_freq': 4, 'gradient_steps': 1, 'target_update_interval': 10000, 'exploration_fraction': 0.1, 'exploration_initial_eps': 1.0, 'exploration_final_eps': 0.01, 'verbose': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN exp1_baseline] Finished: mean_reward=-20.33 ± 0.94 | train_minutes=22.59\n",
      "Saved new best model: exp1_baseline -> models\\best_dqn_pong_cnn.zip (mean_reward=-20.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 8.47GB > 1.60GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RUN exp2_large_batch_slow_lr] Training 50000 steps | hp={'learning_rate': 5e-05, 'gamma': 0.99, 'batch_size': 64, 'buffer_size': 150000, 'train_freq': 4, 'gradient_steps': 1, 'target_update_interval': 8000, 'exploration_fraction': 0.15, 'exploration_initial_eps': 1.0, 'exploration_final_eps': 0.02, 'verbose': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN exp2_large_batch_slow_lr] Finished: mean_reward=-21.00 ± 0.00 | train_minutes=36.96\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.15 GiB for an array with shape (120000, 1, 4, 84, 84) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m best_mean_reward = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     metrics, model = \u001b[43mtrain_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOTAL_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEVAL_EPISODES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     results.append(metrics)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (best_mean_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (metrics[\u001b[33m\"\u001b[39m\u001b[33mmean_reward\u001b[39m\u001b[33m\"\u001b[39m] > best_mean_reward):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain_experiment\u001b[39m\u001b[34m(name, hp, total_timesteps, seed, eval_episodes)\u001b[39m\n\u001b[32m     54\u001b[39m csv_log = os.path.join(\u001b[33m\"\u001b[39m\u001b[33mlogs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining_metrics_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m callback = EpisodeCSVLogger(csv_log, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m model = \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCnnPolicy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[RUN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m steps | hp=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m t0 = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:144\u001b[39m, in \u001b[36mDQN.__init__\u001b[39m\u001b[34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, n_steps, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mself\u001b[39m.exploration_rate = \u001b[32m0.0\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:147\u001b[39m, in \u001b[36mDQN._setup_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._create_aliases()\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:196\u001b[39m, in \u001b[36mOffPolicyAlgorithm._setup_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mYou must pass an environment when using `HerReplayBuffer`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m         replay_buffer_kwargs[\u001b[33m\"\u001b[39m\u001b[33menv\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.env\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28mself\u001b[39m.replay_buffer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.policy = \u001b[38;5;28mself\u001b[39m.policy_class(\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m.observation_space,\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_space,\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_schedule,\n\u001b[32m    210\u001b[39m     **\u001b[38;5;28mself\u001b[39m.policy_kwargs,\n\u001b[32m    211\u001b[39m )\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.policy = \u001b[38;5;28mself\u001b[39m.policy.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:213\u001b[39m, in \u001b[36mReplayBuffer.__init__\u001b[39m\u001b[34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReplayBuffer does not support optimize_memory_usage = True \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand handle_timeout_termination = True simultaneously.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m     )\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m.optimize_memory_usage = optimize_memory_usage\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28mself\u001b[39m.observations = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobs_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optimize_memory_usage:\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# When optimizing memory, `observations` contains also the next observation\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.next_observations = np.zeros((\u001b[38;5;28mself\u001b[39m.buffer_size, \u001b[38;5;28mself\u001b[39m.n_envs, *\u001b[38;5;28mself\u001b[39m.obs_shape), dtype=observation_space.dtype)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.15 GiB for an array with shape (120000, 1, 4, 84, 84) and data type uint8"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run experiments, keep ONLY the best model, and build results table\n",
    "import os\n",
    "TOTAL_TIMESTEPS = 50_000  # increase for better learning (e.g., 500_000 or more)\n",
    "SEED = 42\n",
    "EVAL_EPISODES = 3\n",
    "BEST_MODEL_PATH = os.path.join(\"models\", \"best_dqn_pong_cnn.zip\")\n",
    "\n",
    "results = []\n",
    "best_record = None\n",
    "best_mean_reward = None\n",
    "\n",
    "for exp in experiments:\n",
    "    metrics, model = train_experiment(\n",
    "        name=exp[\"name\"],\n",
    "        hp=exp[\"hp\"],\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        seed=SEED,\n",
    "        eval_episodes=EVAL_EPISODES,\n",
    "    )\n",
    "    results.append(metrics)\n",
    "\n",
    "    if (best_mean_reward is None) or (metrics[\"mean_reward\"] > best_mean_reward):\n",
    "        best_mean_reward = metrics[\"mean_reward\"]\n",
    "        best_record = metrics\n",
    "        # Save/overwrite the single best model\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        model.save(BEST_MODEL_PATH)\n",
    "        print(f\"Saved new best model: {metrics['name']} -> {BEST_MODEL_PATH} (mean_reward={best_mean_reward:.2f})\")\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "# Sort for easier viewing\n",
    "results_df = results_df.sort_values(\"mean_reward\", ascending=False).reset_index(drop=True)\n",
    "print(\"Top result:\")\n",
    "print(results_df.head(1))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d61ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save results table as branis_models.csv (sorted by mean_reward)\n",
    "import os\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "results_csv = os.path.join(\"logs\", \"branis_models.csv\")\n",
    "results_df.to_csv(results_csv, index=False)\n",
    "print(f\"Saved results table to {results_csv}\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d9fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure results_df exists and is sorted by mean_reward\n",
    "_df = results_df.copy()\n",
    "if \"mean_reward\" in _df.columns:\n",
    "    _df = _df.sort_values(\"mean_reward\", ascending=False)\n",
    "\n",
    "# Bar chart: mean_reward by experiment\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(_df[\"name\"], _df[\"mean_reward\"], color=\"#4c78a8\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Mean reward (eval)\")\n",
    "plt.title(\"DQN (CnnPolicy): mean reward by experiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter: train_minutes vs mean_reward\n",
    "if \"train_minutes\" in _df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(_df[\"train_minutes\"], _df[\"mean_reward\"], color=\"#f58518\")\n",
    "    plt.xlabel(\"Train minutes\")\n",
    "    plt.ylabel(\"Mean reward (eval)\")\n",
    "    plt.title(\"Training time vs performance\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Play the BEST saved model with on-screen rendering\n",
    "import os, time\n",
    "import gymnasium as gym\n",
    "import ale_py  # ensure ALE namespace is registered\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(\"models\", \"best_dqn_pong_cnn.zip\")\n",
    "N_EPISODES = 1\n",
    "SEED = 42\n",
    "\n",
    "if not os.path.isfile(BEST_MODEL_PATH):\n",
    "    print(\"Best model not found:\", BEST_MODEL_PATH)\n",
    "    if os.path.isdir(\"models\"):\n",
    "        print(\"Available models:\")\n",
    "        for f in sorted(os.listdir(\"models\")):\n",
    "            if f.endswith(\".zip\"):\n",
    "                print(\" -\", os.path.join(\"models\", f))\n",
    "else:\n",
    "    env = make_atari_env(ENV_ID, n_envs=1, seed=SEED, env_kwargs={\"render_mode\": \"human\"})\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    model = DQN.load(BEST_MODEL_PATH, env=env)\n",
    "    for ep in range(N_EPISODES):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, rewards, dones, infos = env.step(action)\n",
    "            ep_reward += float(rewards[0])\n",
    "            done = bool(dones[0])\n",
    "            time.sleep(1/60)\n",
    "        print(f\"Episode {ep+1} return: {ep_reward:.2f}\")\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
