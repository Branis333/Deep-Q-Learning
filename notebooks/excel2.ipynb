{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjHVyfGpcd28",
        "outputId": "e34154e4-5f94-4ad8-9e58-12c87830442a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXeJSzReclW-",
        "outputId": "6fe06947-21f6-4175-f595-3adc8050b270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated MODEL_DIR: /content/drive/MyDrive/Colab_DQN_Checkpoints/models\n",
            "Updated LOG_DIR: /content/drive/MyDrive/Colab_DQN_Checkpoints/logs\n",
            "Ensured all necessary directories exist in Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory in Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Colab_DQN_Checkpoints'\n",
        "\n",
        "# Modify MODEL_DIR and LOG_DIR to point to the Drive path\n",
        "MODEL_DIR = os.path.join(DRIVE_PATH, \"models\")\n",
        "LOG_DIR = os.path.join(DRIVE_PATH, \"logs\")\n",
        "\n",
        "# Ensure directories are created\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(LOG_DIR, \"tensorboard\"), exist_ok=True)\n",
        "\n",
        "print(f\"Updated MODEL_DIR: {MODEL_DIR}\")\n",
        "print(f\"Updated LOG_DIR: {LOG_DIR}\")\n",
        "print(\"Ensured all necessary directories exist in Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stAZIG1HePWw",
        "outputId": "4960ccfa-a195-4afc-bc48-810a7347403e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.7.0)\n",
            "Requirement already satisfied: ale-py in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from stable-baselines3[extra]) (3.9.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (2.3.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (1.1.1)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.6.1-cp39-cp39-win_amd64.whl (10.6 MB)\n",
            "     ---------------------------------------- 10.6/10.6 MB 3.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: opencv-python in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (4.12.0.88)\n",
            "Requirement already satisfied: pillow in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from stable-baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (2.19.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (14.0.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stable-baselines3[extra]) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ale-py) (4.14.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ale-py) (8.7.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.10.0->ale-py) (3.23.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (80.9.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.8.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.3.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.19.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (6.5.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (4.59.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\excel\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
            "Collecting numpy<3.0,>=1.20\n",
            "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\excel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Installing collected packages: pygame, numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "stable-baselines3 and ale-py re-installed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Excel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\Lib\\\\site-packages\\\\~umpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "WARNING: You are using pip version 22.0.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\Excel\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install stable-baselines3[extra] ale-py\n",
        "print(\"stable-baselines3 and ale-py re-installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvLYVrsJcoAq",
        "outputId": "13a9641a-9c31-4737-a504-57c35bc80ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeriodicSaveCallback class defined successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class PeriodicSaveCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that saves the model periodically to Google Drive.\n",
        "    \"\"\"\n",
        "    def __init__(self, save_freq: int, save_path: str, verbose: int = 0):\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.save_path = save_path\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Check if the current timestep is a multiple of the save frequency\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "            # Construct the save path with the current timestep\n",
        "            path = os.path.join(self.save_path, f\"model_{self.num_timesteps}.zip\")\n",
        "            # Save the model\n",
        "            self.model.save(path)\n",
        "            if self.verbose > 0:\n",
        "                print(f\"Saving model to {path} at timestep {self.num_timesteps}\")\n",
        "        return True\n",
        "\n",
        "print(\"PeriodicSaveCallback class defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BEaHA1chcsUJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import ale_py  # ensure ALE namespace is registered\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "# Cell 6: train_experiment function (saves per-run model & csv)\n",
        "\n",
        "def train_experiment(name: str, hp: Dict, policy: str = \"CnnPolicy\", total_timesteps: int = 50_000, seed: int = 42, eval_episodes: int = 3, save_freq: int = 100_000) -> Tuple[Dict, DQN]:\n",
        "    \"\"\"Train a DQN model with given hyperparameters and save per-run model and CSV.\n",
        "    policy: string policy name to pass to DQN (e.g., 'CnnPolicy' or 'MlpPolicy').\n",
        "    Returns (metrics_dict, model)\n",
        "    \"\"\"\n",
        "    run_model_path = os.path.join(MODEL_DIR, f\"dqn_{name}.zip\")\n",
        "    csv_log = os.path.join(LOG_DIR, f\"training_metrics_{name}.csv\")\n",
        "    tb_log = os.path.join(LOG_DIR, \"tensorboard\", name)\n",
        "\n",
        "    env = make_cnn_env(seed)\n",
        "\n",
        "    # Instantiate callbacks\n",
        "    episode_logger_callback = EpisodeCSVLogger(csv_log, verbose=0)\n",
        "    periodic_save_callback = PeriodicSaveCallback(save_freq=save_freq, save_path=os.path.join(MODEL_DIR, name + \"_checkpoints\"), verbose=1) # Save checkpoints in a subfolder per experiment\n",
        "\n",
        "    # Combine callbacks into a list\n",
        "    callbacks = [episode_logger_callback, periodic_save_callback]\n",
        "\n",
        "    # Create model with the requested policy (CnnPolicy or MlpPolicy)\n",
        "    model = DQN(\n",
        "        policy,\n",
        "        env,\n",
        "        seed=seed,\n",
        "        tensorboard_log=tb_log,\n",
        "        **hp,\n",
        "    )\n",
        "\n",
        "    print(f\"\\n[RUN {name}] Training {total_timesteps} steps | policy={policy} | hp={hp}\")\n",
        "    t0 = time.time()\n",
        "    model.learn(total_timesteps=total_timesteps, callback=callbacks, progress_bar=True)\n",
        "    minutes = (time.time() - t0) / 60.0\n",
        "\n",
        "    mean_r, std_r = evaluate_policy(model, env, n_eval_episodes=eval_episodes)\n",
        "    env.close()\n",
        "\n",
        "    model.save(run_model_path)  # save final model\n",
        "\n",
        "    metrics = {\n",
        "        \"name\": name,\n",
        "        \"mean_reward\": float(mean_r),\n",
        "        \"std_reward\": float(std_r),\n",
        "        \"train_minutes\": minutes,\n",
        "        **hp,\n",
        "        \"policy\": policy,\n",
        "    }\n",
        "    print(f\"[RUN {name}] Finished: mean_reward={mean_r:.2f} \\u00b1 {std_r:.2f} | train_minutes={minutes:.2f}\")\n",
        "    return metrics, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kflYSySScuSA",
        "outputId": "ed381a95-915f-4cd0-ca94-7bfc87a75b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 3 experiment configs (excel).\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Define 10 experiment configurations (prefixed with 'excel_') - tweaked for diversity\n",
        "experiments = [\n",
        "    # 1 Baseline (CNN) - similar to branis but with explicit learning_starts\n",
        "    {\n",
        "        \"name\": \"excel_exp1_baseline\",\n",
        "        \"policy\": \"CnnPolicy\",\n",
        "        \"hp\": dict(learning_rate=1e-4, gamma=0.99, batch_size=32, buffer_size=100_000, train_freq=4, gradient_steps=1, target_update_interval=10_000, learning_starts=5000, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.01, verbose=0),\n",
        "    },\n",
        "    # 2 Larger batch + larger buffer, slightly lower LR\n",
        "    {\n",
        "        \"name\": \"excel_exp2_large_batch\",\n",
        "        \"policy\": \"CnnPolicy\",\n",
        "        \"hp\": dict(learning_rate=7e-5, gamma=0.99, batch_size=64, buffer_size=200_000, train_freq=4, gradient_steps=1, target_update_interval=8000, learning_starts=8000, exploration_fraction=0.12, exploration_initial_eps=1.0, exploration_final_eps=0.02, verbose=0),\n",
        "    },\n",
        "    # 3 More frequent updates (train every step) with small batch\n",
        "    {\n",
        "        \"name\": \"excel_exp3_freq1_small_batch\",\n",
        "        \"policy\": \"CnnPolicy\",\n",
        "        \"hp\": dict(learning_rate=1e-4, gamma=0.99, batch_size=16, buffer_size=100_000, train_freq=1, gradient_steps=1, target_update_interval=5000, learning_starts=2000, exploration_fraction=0.12, exploration_initial_eps=1.0, exploration_final_eps=0.01, verbose=0),\n",
        "    },\n",
        "    # 4 More gradient steps per update\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp4_more_gradsteps\",\n",
        "    #     \"policy\": \"CnnPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=8e-5, gamma=0.99, batch_size=32, buffer_size=150_000, train_freq=4, gradient_steps=4, target_update_interval=8000, learning_starts=5000, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.01, verbose=0),\n",
        "    # },\n",
        "    # 5 Higher gamma (longer horizon)\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp5_high_gamma\",\n",
        "    #     \"policy\": \"CnnPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=1e-4, gamma=0.997, batch_size=32, buffer_size=120_000, train_freq=4, gradient_steps=1, target_update_interval=7000, learning_starts=5000, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.01, verbose=0),\n",
        "    # },\n",
        "    # 6 Small buffer, faster target updates\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp6_small_buffer_fast_target\",\n",
        "    #     \"policy\": \"CnnPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=1.2e-4, gamma=0.99, batch_size=32, buffer_size=50_000, train_freq=4, gradient_steps=1, target_update_interval=4000, learning_starts=2000, exploration_fraction=0.2, exploration_initial_eps=1.0, exploration_final_eps=0.02, verbose=0),\n",
        "    # },\n",
        "    # 7 MLP policy (for comparison) - smaller network via policy_kwargs\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp7_mlp_small\",\n",
        "    #     \"policy\": \"MlpPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=5e-4, gamma=0.99, batch_size=64, buffer_size=100_000, train_freq=4, gradient_steps=1, target_update_interval=10000, learning_starts=5000, exploration_fraction=0.15, exploration_initial_eps=1.0, exploration_final_eps=0.02, verbose=0, policy_kwargs=dict(net_arch=[256, 256])),\n",
        "    # },\n",
        "    # 8 MLP larger (deeper) - see if MLP can learn with stacked frames\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp8_mlp_deep\",\n",
        "    #     \"policy\": \"MlpPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=3e-4, gamma=0.99, batch_size=64, buffer_size=150_000, train_freq=4, gradient_steps=2, target_update_interval=8000, learning_starts=8000, exploration_fraction=0.15, exploration_initial_eps=1.0, exploration_final_eps=0.02, verbose=0, policy_kwargs=dict(net_arch=[512, 512])),\n",
        "    # },\n",
        "    # 9 Aggressive exploration decay (faster exploitation)\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp9_quick_decay\",\n",
        "    #     \"policy\": \"CnnPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=1e-4, gamma=0.99, batch_size=32, buffer_size=120_000, train_freq=4, gradient_steps=1, target_update_interval=8000, learning_starts=4000, exploration_fraction=0.03, exploration_initial_eps=1.0, exploration_final_eps=0.01, verbose=0),\n",
        "    # },\n",
        "    # 10 Conservative LR + gradient clipping (max_grad_norm) to stabilize training\n",
        "    # {\n",
        "    #     \"name\": \"excel_exp10_slow_lr_clip\",\n",
        "    #     \"policy\": \"CnnPolicy\",\n",
        "    #     \"hp\": dict(learning_rate=5e-5, gamma=0.99, batch_size=32, buffer_size=150_000, train_freq=4, gradient_steps=1, target_update_interval=8000, learning_starts=8000, exploration_fraction=0.12, exploration_initial_eps=1.0, exploration_final_eps=0.01, verbose=0, max_grad_norm=10),\n",
        "    # },\n",
        "]\n",
        "\n",
        "print(f\"Prepared {len(experiments)} experiment configs (excel).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o5Uzl2k1cwrn"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Helper to make CNN env with frame stack\n",
        "def make_cnn_env(seed: int, render_mode: str = None):\n",
        "    # render_mode can be None or 'human' for play cell\n",
        "    env = make_atari_env(ENV_ID, n_envs=1, seed=seed, env_kwargs={\"render_mode\": render_mode} if render_mode else None)\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R8HUFHvWc-h1"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Imports and constants\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import ale_py  # ensure ALE namespace is registered\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "\n",
        "# Constants (edit as needed)\n",
        "ENV_ID = \"ALE/Pong-v5\"\n",
        "MODEL_DIR = os.path.join(\"models\")\n",
        "LOG_DIR = os.path.join(\"logs\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(LOG_DIR, \"tensorboard\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfsFqL-fdApg"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# Define the base directory in Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Colab_DQN_Checkpoints'\n",
        "\n",
        "# Modify MODEL_DIR and LOG_DIR to point to the Drive path\n",
        "MODEL_DIR = os.path.join(DRIVE_PATH, \"models\")\n",
        "LOG_DIR = os.path.join(DRIVE_PATH, \"logs\")\n",
        "\n",
        "# Ensure directories are created\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(LOG_DIR, \"tensorboard\"), exist_ok=True)\n",
        "\n",
        "print(f\"Updated MODEL_DIR: {MODEL_DIR}\")\n",
        "print(f\"Updated LOG_DIR: {LOG_DIR}\")\n",
        "print(\"Ensured all necessary directories exist in Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w18hYdPJdDMa"
      },
      "outputs": [],
      "source": [
        "# Cell 3: EpisodeCSVLogger (same behavior as in branis.ipynb)\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class EpisodeCSVLogger(BaseCallback):\n",
        "    def __init__(self, csv_path: str, verbose: int = 0):\n",
        "        super().__init__(verbose)\n",
        "        self.csv_path = csv_path\n",
        "        self.rows = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        for info in self.locals.get(\"infos\", []):\n",
        "            if \"episode\" in info:\n",
        "                ep = info[\"episode\"]\n",
        "                # 'l' = length, 'r' = reward in VecEnv episode info\n",
        "                self.rows.append((self.num_timesteps, ep.get(\"l\", None), ep.get(\"r\", None)))\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        import csv, os\n",
        "        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n",
        "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\"timestep\", \"ep_length\", \"ep_reward\"])\n",
        "            w.writerows(self.rows)\n",
        "        if self.verbose:\n",
        "            print(f\"Saved episode CSV to {self.csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUW8frgRdGre"
      },
      "outputs": [],
      "source": [
        "TOTAL_TIMESTEPS = 1500000  # change for longer runs\n",
        "SEED = 42\n",
        "EVAL_EPISODES = 3\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, \"excel_best_dqn.zip\")\n",
        "CHECKPOINT_SAVE_FREQ = 100_000 # Save a checkpoint every 100,000 timesteps\n",
        "\n",
        "results = []\n",
        "best_mean_reward = None\n",
        "best_record = None\n",
        "\n",
        "for exp in experiments:\n",
        "    policy = exp.get(\"policy\", \"CnnPolicy\")\n",
        "    metrics, model = train_experiment(name=exp[\"name\"], hp=exp[\"hp\"], policy=policy, total_timesteps=TOTAL_TIMESTEPS, seed=SEED, eval_episodes=EVAL_EPISODES, save_freq=CHECKPOINT_SAVE_FREQ)\n",
        "    results.append(metrics)\n",
        "\n",
        "    if (best_mean_reward is None) or (metrics[\"mean_reward\"] > best_mean_reward):\n",
        "        best_mean_reward = metrics[\"mean_reward\"]\n",
        "        best_record = metrics\n",
        "        model.save(BEST_MODEL_PATH)\n",
        "        print(f\"Saved new best model: {metrics['name']} -> {BEST_MODEL_PATH} (mean_reward={best_mean_reward:.2f})\")\n",
        "\n",
        "# Save summary CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(\"mean_reward\", ascending=False).reset_index(drop=True)\n",
        "results_csv = os.path.join(LOG_DIR, \"excel_models.csv\")\n",
        "results_df.to_csv(results_csv, index=False)\n",
        "print(f\"Saved results table to {results_csv}\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "uvIWMMjciygX",
        "outputId": "3ae30453-5154-46c3-8dac-78a87084279a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook dir: c:\\Users\\Excel\\Desktop\\Github Projects\\Deep-Q-Learning\\notebooks\n",
            "Project root: c:\\Users\\Excel\\Desktop\\Github Projects\\Deep-Q-Learning\n",
            "Loading best model from: c:\\Users\\Excel\\Desktop\\Github Projects\\Deep-Q-Learning\\models\\Excel_model\\excel_best_dqn.zip\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 2.63 GiB for an array with shape (100000, 1, 4, 84, 84) and data type uint8",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m env \u001b[38;5;241m=\u001b[39m make_atari_env(ENV_ID, n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39mSEED, env_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     28\u001b[0m env \u001b[38;5;241m=\u001b[39m VecFrameStack(env, n_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBEST_MODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPISODES):\n\u001b[0;32m     31\u001b[0m     obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
            "File \u001b[1;32mc:\\Users\\Excel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:740\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(data)\n\u001b[0;32m    739\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 740\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# put state_dicts back in place\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_parameters(params, exact_match\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Excel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:147\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Excel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:196\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass an environment when using `HerReplayBuffer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    200\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    201\u001b[0m         n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs,\n\u001b[0;32m    202\u001b[0m         optimize_memory_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_memory_usage,\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule,\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs,\n\u001b[0;32m    211\u001b[0m )\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Excel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:213\u001b[0m, in \u001b[0;36mReplayBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplayBuffer does not support optimize_memory_usage = True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand handle_timeout_termination = True simultaneously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_memory_usage \u001b[38;5;241m=\u001b[39m optimize_memory_usage\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optimize_memory_usage:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When optimizing memory, `observations` contains also the next observation\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_observations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_shape), dtype\u001b[38;5;241m=\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mdtype)\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.63 GiB for an array with shape (100000, 1, 4, 84, 84) and data type uint8"
          ]
        }
      ],
      "source": [
        "# Cell 9: Play the BEST saved model (render on-screen)\n",
        "import time\n",
        "from pathlib import Path\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# Get the notebook's current directory and go back 2 levels to project root\n",
        "notebook_dir = Path.cwd()\n",
        "project_root = notebook_dir.parent\n",
        "BEST_MODEL_PATH = project_root / \"models\" / \"Excel_model\" / \"excel_best_dqn.zip\"\n",
        "\n",
        "print(f\"Notebook dir: {notebook_dir}\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Loading best model from: {BEST_MODEL_PATH}\")\n",
        "\n",
        "ENV_ID = \"ALE/Pong-v5\"\n",
        "N_EPISODES = 1\n",
        "SEED = 42\n",
        "\n",
        "if not os.path.isfile(BEST_MODEL_PATH):\n",
        "    print(\"Best model not found:\", BEST_MODEL_PATH)\n",
        "    if os.path.isdir(MODEL_DIR):\n",
        "        print(\"Available models:\")\n",
        "        for f in sorted(os.listdir(MODEL_DIR)):\n",
        "            if f.endswith(\".zip\"):\n",
        "                print(\" -\", os.path.join(MODEL_DIR, f))\n",
        "else:\n",
        "    env = make_atari_env(ENV_ID, n_envs=1, seed=SEED, env_kwargs={\"render_mode\": \"human\"})\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    model = DQN.load(BEST_MODEL_PATH, env=env)\n",
        "    for ep in range(N_EPISODES):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, rewards, dones, infos = env.step(action)\n",
        "            ep_reward += float(rewards[0])\n",
        "            done = bool(dones[0])\n",
        "            time.sleep(1/60)\n",
        "        print(f\"Episode {ep+1} return: {ep_reward:.2f}\")\n",
        "    env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
